{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de27e552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://androidkt.com/use-saved-pytorch-model-to-predict-single-and-multiple-images/\n",
    "# https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-predict-new-samples-with-your-pytorch-model.mdS\n",
    "# https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6\n",
    "# https://www.kaggle.com/code/andradaolteanu/pytorch-rnns-and-lstms-explained-acc-0-99#3.-RNN-with-1-Layer-%F0%9F%93%98\n",
    "# https://www.kaggle.com/code/omershect/learning-pytorch-lstm-deep-learning-with-m5-data/comments\n",
    "# https://curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7376709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Device is:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Available Device is: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b69f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./robotdatacollection3/ep2.csv')\n",
    "feature_list = ['Fx','Fy','Fz','Mx','My']\n",
    "TIMESTEP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530ef679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before adding dimension: (5351, 5)\n",
      "Shape AFTER adding dimension: (5351, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "df_new = df[feature_list]\n",
    "print('Shape before adding dimension:',df_new.shape)\n",
    "df_new = np.expand_dims(df_new, axis=1)\n",
    "print('Shape AFTER adding dimension:',df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17696a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input has format of:\n",
      "    sample number = 5351 \n",
      "    window size = 1 \n",
      "    feature number= 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"LSTM input has format of:\n",
    "    sample number = {df_new.shape[0]} \n",
    "    window size = {df_new.shape[1]} \n",
    "    feature number= {df_new.shape[2]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982eaab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5351, 1, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91b850",
   "metadata": {},
   "source": [
    "### Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11633815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the data in a window form, in case of timestep = 5\n",
    "# i=0: X_0=[x(0)..x(5)], y=x(6)\n",
    "\n",
    "def to_sequence(data, timesteps=1):\n",
    "    n_features=data.shape[2]\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-timesteps):\n",
    "        # takes a window of data of specified timesteps\n",
    "        \n",
    "        _x = data[i:(i+timesteps)]\n",
    "        _x = _x.reshape(timesteps, n_features)\n",
    "#         print(_x.shape)\n",
    "        _y = data[i+timesteps]\n",
    "        _y = _y.reshape(n_features)\n",
    "#         print(_y.shape)\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc4fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = to_sequence(df_new, timesteps=TIMESTEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06db0a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5301, 50, 5)\n",
      "(5301, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35d18f",
   "metadata": {},
   "source": [
    "**Pytorch uses Tensors as inputs to the model, so we convert numpy to Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456895c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = torch.Tensor(x_train)\n",
    "trainy = torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a0c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5301, 50, 5])\n",
      "torch.Size([5301, 5])\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1214298",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "\n",
    "LSTM layers work on 3D data with the following structure `(sequence/batch, timestep, feature)`.\n",
    "\n",
    "* **input_dim**: Number of samples in the data. In our case we do not use batches but instead one sample at a time. Each sample has TIMESTEP size of data in it. \n",
    "* **hidden_dim**: Number of LSTM cells\n",
    "* **layer_dim**: Number of LSTM hidden layers\n",
    "* **output_dim**: Number of outputs = feature number (in case of regression task)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9bc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM: extensions that can improve performance. They train model forward and backward on the same input\n",
    "# much slower, good for NLP \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__() # inherit from nn.Module\n",
    "        \n",
    "        # hidden dims\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # building LSTM\n",
    "        # setting batch_first=True: input and output order: (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('input shape',x.shape)\n",
    "        # initialize hidden state with zeros\n",
    "        hidden_state = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        # initialize cell state\n",
    "        cell_state = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "    \n",
    "        \n",
    "        # we need to detach tensor from current graph\n",
    "        # if we do not, we will backpop all the way to the start even after going through another batch\n",
    "        out, (last_hidden_state, last_cell_state) = self.lstm(x, (hidden_state.detach(), cell_state.detach()))\n",
    "#         print('output shape', out.shape)\n",
    "        \n",
    "        # index hidden state of last time step\n",
    "        # out.size() -> 100,28,100\n",
    "        # out[:,-1,:] -> 100, 100-> just want last time step hidden states\n",
    "        \n",
    "        # rehape\n",
    "        out = self.fc(out[:,-1,:])\n",
    "#         print('reshape',out.shape)\n",
    "        # out.size() --> 100,10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dfe7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5# 50 # timestep\n",
    "hidden_dim = 15 # LSTM layer 5 units\n",
    "layer_dim = 1 # one LSTM layer\n",
    "output_dim = 5 # we outout one prediction at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c53414",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "# lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "093e8067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(5, 15, batch_first=True)\n",
       "  (fc): Linear(in_features=15, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ebf7d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 6.034580707550049\n",
      "Epoch 10, Loss 5.793862819671631\n",
      "Epoch 20, Loss 5.539210319519043\n",
      "Epoch 30, Loss 5.250224590301514\n",
      "Epoch 40, Loss 4.906777858734131\n",
      "Epoch 50, Loss 4.503946304321289\n",
      "Epoch 60, Loss 4.058081150054932\n",
      "Epoch 70, Loss 3.5879971981048584\n",
      "Epoch 80, Loss 3.100661039352417\n",
      "Epoch 90, Loss 2.6243836879730225\n"
     ]
    }
   ],
   "source": [
    "# regression, calculates the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# updates the weights and biases to reduce loss\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training LSTM for each epoch we\n",
    "for epoch in range(100):\n",
    "    \n",
    "    # enable train mode\n",
    "    lstm_model.train(True)\n",
    "    \n",
    "    # clear gradients with respect to params, always before backprop\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(True):\n",
    "        \n",
    "        # forward pass to get outputs/ make predictions for that batch\n",
    "        output = lstm_model(trainX)\n",
    "        # calculate loss and its gradients\n",
    "        loss = criterion(output, trainy)\n",
    "        # getting gradients w.r.t. params\n",
    "        loss.backward()\n",
    "\n",
    "        #updating params/ weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4c5066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 5])\n",
      "\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.002825021743774414\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0012857913970947266\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0009996891021728516\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0008528232574462891\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0007534027099609375\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0007054805755615234\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0006396770477294922\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0006031990051269531\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0005667209625244141\n",
      "tensor([[-0.2265,  0.4771,  1.4639, -0.0226,  0.1733]])\n",
      "dt 0.0005071163177490234\n"
     ]
    }
   ],
   "source": [
    "test_x = np.ones((1,50,5))\n",
    "test_x = torch.Tensor(test_x)\n",
    "print(test_x.shape)\n",
    "print()\n",
    "\n",
    "t = 0\n",
    "times_dnn = []\n",
    "lstm_model.eval()\n",
    "torch.no_grad()\n",
    "\n",
    "while t < 10:\n",
    "    t_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = lstm_model(test_x)\n",
    "        print(y_pred)\n",
    "    dt = time.time()-t_start\n",
    "    print('dt',dt)\n",
    "    \n",
    "    times_dnn.append(dt)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7f3a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5])\n",
      "\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0020952224731445312\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0003695487976074219\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0002911090850830078\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0002658367156982422\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.00025153160095214844\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.00022935867309570312\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0002288818359375\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0004115104675292969\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.0002067089080810547\n",
      "tensor([[-0.1676,  0.2939, -0.0378,  0.0750,  0.0078]])\n",
      "dt 0.00020003318786621094\n"
     ]
    }
   ],
   "source": [
    "test_x = np.ones((1,1,5))\n",
    "test_x = torch.Tensor(test_x)\n",
    "print(test_x.shape)\n",
    "print()\n",
    "\n",
    "t = 0\n",
    "times_dnn = []\n",
    "lstm_model.eval()\n",
    "torch.no_grad()\n",
    "\n",
    "while t < 10:\n",
    "    t_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = lstm_model(test_x)\n",
    "        print(y_pred)\n",
    "    dt = time.time()-t_start\n",
    "    print('dt',dt)\n",
    "    \n",
    "    times_dnn.append(dt)\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae278f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af59c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1339b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
